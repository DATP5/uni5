## Ex 1
![[Pasted image 20241101090655.png]]
![[Pasted image 20241101090811.png]]
- (1st & 2nd q) Given that our project is a product of experiments, where the code quality is not of any importance, the regularly touted quality attributes are of a lesser priority. Here, quality attributes, to be relevant, should be more about what allows us to iterate quickly, than about what produces a quality product. For the product, reliability, adaptability and reusability matters since we are trying to generalise over multiple objects in our ai.
- We measure quality specifically on how well the product performs its end task, i.e. how many chips it can generalise across and how well, compared to how much labeled input data we use. That is general enough to apply to any product which solves our problem at hand, as they are not tied specifically to the solution.
- Given our experimental apporach we do not have traditional requirements, other than certain attributes we try to optimise. The actual validation is a process of showing the result to samsung and having them give feedback.
- Documentation is somewhat non-existant, following the agile way of documenting only what is required. This is done since fast iteration would mean most documentation we write is obsolete within a week or 2.
## Ex 2
![[Pasted image 20241101090715.png]]
- At the moment we do not have a proper review process. The plan should be to ad hoc have someone else look at all architecture changes made, either before or after training a model on it, as a formal time for review makes little sense with what we are working on. Testing comes somewhat naturally, in that every model, when run, produces a set of numbers describing how well it performs, and internal testing isn't relevant. . The artifacts tested on are the trained models, but those are tested the moment they are produced, and currently discarded afterwards.
- we do not currently have a product roadmap, meaning these exercises are kind of hard to complete. 