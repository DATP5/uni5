## Ex 1
![[Pasted image 20241108090938.png]]
- Duplicate code, while it may make understandig harder, is not too important to a project which will not have to live for long, as we switch models quickly
- Mysterious names make it harder to understand for those not involved in pytorch ml coding already, as it has a lot of its own conventions you have to learn. After that though, it is a non-issue
- Large class - dataloaders which load, manipulates, & augments data at once
- Shotgun surgery - certain changes, fx window size, requires changing config file but also preprocessor code and calculating other related config values, which probably should have been derived in the first place
 - Data clumps - Data is shuffled without labels, requiring generation of data again for supervised learning 
## Ex 2
![[Pasted image 20241108091751.png]]

| Quadrant | Strategy, What should we do in our project                                                                                                                                                                                                                                                                             |
| -------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Q1       | Preprocessor / dataloading really should be tested to confirm it only shuffles when we want to, etc. this has been a repeated issue, which could have been eliminated with proper testing.<br>A lot of internal things to the model is harder to test, though non-learned  components probably could have been tested. |
| Q2       | Testing of models probably fall in this category, although it comes naturally as the "test" is simply running the program and writing down the result.                                                                                                                                                                 |
| Q3       | Exploratory testing when we plot things and see what happens, which have been a major way of confirming different  behaviour this semester. Thank god for matplotlib.                                                                                                                                                  |
| Q4       | Testing performance as in how fast we can run compared to the size of the input, which right now may be problematic. Also performance in how well it performs its operations, plotted against each other.                                                                                                              |

## Ex 3
![[Pasted image 20241108093244.png]]
Q1 is verification testing, with Q2 and Q4 in testing how well the model performs. Validation of whether it is acutally usable, is done in weekly meetings with samsung, where we discuss what we have tried and what we are attempting to identify

## Ex 4
![[Pasted image 20241108093548.png]]
Verification happens during the sprint itself as a continuous effort with produced tests, while validation is the primary focus of the sprint review, discussing progress with the customer, or PO in a pinch. As such, developers and to an extent scrum master are responsible for the continuous verification, while validation is a responsibility shared with PO.

## Ex 5
![[Pasted image 20241108093908.png]]
Validation differs in XP, as the customer is always present to discuss what is required
Verification has the elements from scrum, but is also facilitated by pair programming, with forced CI/CD on all changes